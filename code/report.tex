\documentclass[letter,11pt]{article}
\usepackage[utf8x]{inputenc}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage{url}

%opening
\title{Using NLP Techniques for File Fragment Classification}
\author{Simran Fitzgerald, George Mathews, Colin Morris and Oles Zhulyn}

\begin{document}

\maketitle

\begin{abstract}
The classification of file fragments is an important problem in digital forensics. The literature does not include a lot of work that involves applying machine learning techniques to this problem. In this work, we explore the use of techniques from natural language processing to classify file fragments. We took a supervised learning approach, based on the use of support vector machines combined with the bag-of-words model, where text documents are represented as unordered bags of words. This technique has been repeatedly shown to be effective and robust in classifying text documents (e.g., in distinguishing positive movie reviews from negative ones).

In our approach, we represent file fragments as ``bags of bytes'' with feature vectors consisting of unigram and bigram counts, as well as other statistical measurements (including entropy and others). We made use of the publicly available Garfinkel data corpus to generate file fragments for training and testing. We ran a series of experiments, and found that this approach is effective and robust in this domain as well.
\end{abstract}

\section{Introduction}
\label{Section:Introduction}
The classification of file fragments is an important problem in digital forensics, particularly for the purpose of carving fragmented files. Files that are not stored contiguously on the hard drive must be carefully reconstructed from fragments based on their content during file carving. Because the search space for fragments belonging to a particular file is so large, it is essential to have an automated method for distinguishing whether a fragment potentially belongs to a file or not. For example, a fragment from a plain-text file (e.g. \texttt{txt}) certainly does not belong to a compressed image file (e.g. \texttt{jpg}). Garfinkel \cite{Garfinkel07} observes that although the fragmentation of files is relatively rare on today's file systems, the files of interest in forensic investigations are more likely to be fragmented than other files. For example, large files that have been modified numerous times over a long period of time on a hard drive that is filled near capacity will likely exhibit fragmentation. In this work, we explore the application of machine learning techniques from natural language processing to the problem of file fragment classification.

Classification is a standard machine learning problem. There has been work done on applying machine learning techniques to the problem of file fragment classification \cite{Axelsson10, Conti10, Li10, Veenman07}. However, the body of work in the literature is not exhaustive. Our contribution is the application of supervised machine learning techniques used in natural language processing to this problem.

In previous work, the histograms of the bytes within file fragments are used for classification \cite{Li10, Veenman07}. In natural language processing, this kind of approach is called the ``bag-of-words model'', where text documents are represented as unordered bags of words. The single word tokens are called unigrams, but tokens consisting of any fixed number of words can also be considered. Two word tokens are called bigrams, and bigram counts capture more information about the structure of the data being classified than do unigram counts alone. Combined with various machine learning techniques, this kind of approach has been repeatedly shown to be effective and robust in classifying text documents (e.g. in determining whether a piece of text has a positive or negative sentiment). In our approach, we consider the unigram and bigram counts of the bytes within file fragments, along with other statistical measurements, to generate feature vector representations of the file fragments, which we then classify based on 23 different file types using a support vector machine.

Support vector machines are supervised machine learning algorithms that are very effective for classification problems \cite{Li10}. During the training phase, a support vector machine partitions a high-dimensional space based on the points it contains that belong to known classes. File fragments can be represented in the high-dimensional space by being transformed into feature vectors. During the testing phase, file fragments of unknown types are transformed into feature vectors and are classified according to what partition they lie in in the high-dimensional space. We make use of the \texttt{libsvm} \cite{CC01a} library, which is the most widely used implementation of support vector machines, to perform our experiments.

Most of the previous work on this problem exclusively uses private data sets, making it more difficult for other researchers to reproduce experimental results. We follow the example of Axelsson \cite{Axelsson10} and derive the data set we use for training and testing from the freely available corpus of forensics research data by Garfinkel et al. \cite{Garfinkel09} (the \texttt{govdocs1} data set described in Section 4 of the cited paper). We determined the most well-represented file types in the data set and selected 23 of them based on how well-known they are. For each of the 23 file types supported by our classifier, we downloaded files uniformly at random from the \texttt{govdocs1} data set such that we would have at least 10 files made up of at least 10000 512-byte fragments. From this, we uniformly at random selected 9000 fragments for each file type to create our data set. When generating the fragments, we omitted the first and last fragments of each file, as the first fragment frequently contains header information that identifies the file type, and the last fragment might not be 512 bytes in length.

\textbf{TODO mention experimental setup and hint at results}

The paper is organized as follows. In Section \ref{Section:RelatedWork}, we provide a brief overview of related work done in this area. In Section \ref{Section:ExperimentalSetup}, we describe our experimental setup, which includes how we generated the data set we used in training and testing, as well as details about the features we used in our feature vectors. In Section \ref{Section:Results}, we present our results. We conclude and suggest future directions for this work in Section \ref{Section:ConclusionAndFutureWork}.

\section{Related work}
\label{Section:RelatedWork}
TODO \cite{Conti10, Garfinkel09, Axelsson10, Calhoun08, Li10, Veenman07}

\section{Experimental setup}
\label{Section:ExperimentalSetup}
TODO

\section{Results}
\label{Section:Results}
TODO

\section{Conclusion and future work}
\label{Section:ConclusionAndFutureWork}
TODO

\section*{Acknowledgements}
We are grateful to G. Scott Graham for suggesting the initial direction for this project, and to Greg Conti for his help.


% References
\newpage
\bibliographystyle{plain}
\bibliography{report}

\end{document}
