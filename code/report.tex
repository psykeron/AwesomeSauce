\documentclass[letter,11pt]{article}
\usepackage[utf8x]{inputenc}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage{url}

%opening
\title{Using NLP Techniques for File Fragment Classification}
\author{Simran Fitzgerald, George Mathews, Colin Morris and Oles Zhulyn}

\begin{document}

\maketitle

\begin{abstract}
The classification of file fragments is an important problem in digital forensics. The literature does not include comprehensive work on applying machine learning techniques to this problem. In this work, we explore the use of techniques from natural language processing to classify file fragments. We took a supervised learning approach, based on the use of support vector machines combined with the bag-of-words model, where text documents are represented as unordered bags of words. This technique has been repeatedly shown to be effective and robust in classifying text documents (e.g., in distinguishing positive movie reviews from negative ones).

In our approach, we represent file fragments as ``bags of bytes'' with feature vectors consisting of unigram and bigram counts, as well as other statistical measurements (including entropy and others). We made use of the publicly available Garfinkel data corpus to generate file fragments for training and testing. We ran a series of experiments, and found that this approach is effective and robust in this domain as well.
\end{abstract}

\section{Introduction}
\label{Section:Introduction}
The classification of file fragments is an important problem in digital forensics, particularly for the purpose of carving fragmented files. Files that are not stored contiguously on the hard drive must be carefully reconstructed from fragments based on their content during file carving. Because the search space for fragments belonging to a particular file is so large, it is essential to have an automated method for distinguishing whether a fragment potentially belongs to a file or not. For example, a fragment from a plain-text file (e.g. \texttt{txt}) certainly does not belong to a compressed image file (e.g. \texttt{jpg}). Garfinkel \cite{Garfinkel07} observes that although the fragmentation of files is relatively rare on today's file systems, the files of interest in forensic investigations are more likely to be fragmented than other files. For example, large files that have been modified numerous times over a long period of time on a hard drive that is filled near capacity will likely exhibit fragmentation. In this work, we explore the application of machine learning techniques from natural language processing to the problem of file fragment classification.

Classification is a standard machine learning problem. There has been work done on applying machine learning techniques to the problem of file fragment classification \cite{Axelsson10, Conti10, Li10, Veenman07}. However, the body of work in the literature is not exhaustive. Our contribution is the application of supervised machine learning techniques used in natural language processing to this problem.

In previous work, the histograms of the bytes within file fragments are used for classification \cite{Li05, Li10, Stolfo05, Veenman07}. In natural language processing, this kind of approach is called the ``bag-of-words model'', where text documents are represented as unordered bags of words. The single word tokens are called unigrams, but tokens consisting of any fixed number of words can also be considered. Two word tokens are called bigrams, and bigram counts capture more information about the structure of the data being classified than do unigram counts alone. Combined with various machine learning techniques, this kind of approach has been repeatedly shown to be effective and robust in classifying text documents (e.g. in determining whether a piece of text has a positive or negative sentiment). In our approach, we consider the unigram and bigram counts of the bytes within file fragments, along with other statistical measurements, to generate feature vector representations of the file fragments, which we then classify based on 23 different file types using a support vector machine.

Support vector machines are supervised machine learning algorithms that are very effective for classification problems \cite{Li10}. During the training phase, a support vector machine partitions a high-dimensional space based on the points it contains that belong to known classes. File fragments can be represented in the high-dimensional space by being transformed into feature vectors. During the testing phase, file fragments of unknown types are transformed into feature vectors and are classified according to what partition they lie in in the high-dimensional space. We make use of the \texttt{libsvm} \cite{CC01a} library, which is the one of the most widely used implementations of support vector machines, to perform our experiments.

Most of the previous work on this problem exclusively uses private data sets, making it more difficult for other researchers to reproduce experimental results. We follow the example of Axelsson \cite{Axelsson10} and derive the data set we use for training and testing from the freely available corpus of forensics research data by Garfinkel et al. \cite{Garfinkel09} (the \texttt{govdocs1} data set described in Section 4 of the cited paper). We determined the most well-represented file types in the data set and selected 23 of them based on how well-known they are. For each of the 23 file types supported by our classifier, we downloaded files uniformly at random from the \texttt{govdocs1} data set such that we would have at least 10 files made up of at least 10000 512-byte fragments. From this, we uniformly at random selected 9000 fragments for each file type to create our data set. When generating the fragments, we omitted the first and last fragments of each file, as the first fragment frequently contains header information that identifies the file type, and the last fragment might not be 512 bytes in length.

\textbf{TODO mention experimental setup and hint at results}

The paper is organized as follows. In Section \ref{Section:RelatedWork}, we provide a brief overview of related work done in this area. In Section \ref{Section:ExperimentalSetup}, we describe our experimental setup, which includes how we generated the data set we used in training and testing, as well as details about the features we used in our feature vectors. In Section \ref{Section:Results}, we present our results. We conclude and suggest future directions for this work in Section \ref{Section:ConclusionAndFutureWork}.

\section{Related work}
\label{Section:RelatedWork}
Previous work that explores the application of machine learning techniques to the problem of file fragment classification appears in the literature. \cite{Li10, Veenman07}

Calhoun and Coles \cite{Calhoun08}

Axelsson \cite{Axelsson10} considered 28 different file types and applied the k-nearest-neighbors classification technique with nearest compression distance as the distance metric between file fragments. The file fragment data was generated from the freely available \texttt{govdocs1} corpus by Garfinkel et al. \cite{Garfinkel09}. Axelsson's experiments consisted of ten trials. In each trial, 10 files were selected at random (with the types of the files uniformly distributed) and 14 512-byte fragments were extracted at random from each of them. The fragments were then classified against a data set of approximately 3000 file fragments with known types. The classification accuracy was around 34\%, with higher accuracy being achieved for file fragments with lower entropy.

Conti et al. \cite{Conti10} made use of a private data set of 14000 1024-byte binary fragments which they characterized by vectors of statistical measurements (namely, Shannon entropy, Hamming weight, Chi-square goodness-of-fit, and mean byte value). They classified each of these vectors against the remaining 13999 using k-nearest-neighbors with Euclidean distance as the distance metric. They achieved 98.55\% classification accuracy for Random/Compressed/Encrypted fragments, 100\% for Base64 Encoded fragments, 100\% for Unencoded fragments, 96.7\% for Machine Code (ELF and PE) fragments, 98.7\% for Text fragments, and 82.5\% for Bitmap fragments. However, the classifier did not perform as well when applied to real-world binary data, especially when it contained fragments of ``a previously unstudied primitive type, even one with a closely related structure''. They also classified the fragments according to types of a very coarse granularity.




\section{Experimental setup}
\label{Section:ExperimentalSetup}
The data set we used for training and testing is derived from the freely available corpus of forensics research data by Garfinkel et al. \cite{Garfinkel09} (the \texttt{govdocs1} data set described in Section 4 of the cited paper). We determined the most well-represented file types in the data set and selected 23 of them based on how well-known they are to us. The first of these criteria ensured that we acquired a good variety of file fragment data for each file type. The second of these criteria is not a rigorous one. Although we aimed to get a good representation of file types that are likely to be of forensic interest, a rigorous methodology for selecting the most appropriate file types is outside the scope of this paper. Nevertheless, most of the file types we selected overlap with the ones selected by Axelsson \cite{Axelsson10} who made use of the same Garfinkel corpus to derive his data set.

After selecting the file types, we proceeded to download files uniformly at random from the \texttt{govdocs1} corpus such that we would have at least 10 files made up of at least 10000 512-byte fragments for each of the 23 file types. Because the files in the \texttt{govdocs1} corpus are of variable length, and files from different file types are not equally represented, it was necessary to download more than 10 files or files consisting of more than 10000 fragments, for each of the file types, in order to meet both criteria. From this, we uniformly at random selected 9000 fragments for each file type to create our data set. When generating the fragments, we omitted the first and last fragments of each file, as the first fragment frequently contains header information that identifies the file type, and the last fragment might not be 512 bytes in length. Calhoun and Coles \cite{Calhoun08}, and Conti et al. \cite{Conti10} omit these fragments as well. This approach enabled us to generate a large data set of file fragments with an equal number of file fragments for each file type, and with each fragment being derived from a variety of files with the same type.

\textbf{TODO finish this!!!!!!}

\section{Results}
\label{Section:Results}
\textbf{TODO finish this!!!!!!}

\section{Conclusion and future work}
\label{Section:ConclusionAndFutureWork}
File fragment classification is an important problem in digital forensics. For this paper, we explored the application of supervised machine learning techniques from natural language processing to this problem. We generated a large data set of file fragments for 23 different file types, which we derived from the freely available \texttt{govdocs1} corpus by Garfinkel et al. \cite{Garfinkel09}. We represented each file fragment with a feature vector consisting of the unigram and bigram counts of bytes in the fragment, along with several other statistical measurements (such as entropy). Our file fragment classification approach consisted of using a support vector machine along with these feature vectors. We ran several experiments, and found this to be an effective and robust approach.

\textbf{TODO when experiments are done!!}

\section*{Acknowledgements}
We are grateful to G. Scott Graham for suggesting the initial direction for this project, and to Greg Conti for his help.


% References
\newpage
\bibliographystyle{plain}
\bibliography{report}

\end{document}
